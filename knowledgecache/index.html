<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="turbo-cache-control" content="no-cache" data-turbo-track="reload" data-track-token="3.8.1.800899148656">

    <!-- See retype.com -->
    <meta name="generator" content="Retype 3.8.1">

    <!-- Primary Meta Tags -->
    <title>Membria: Knowledge Cache Graph (KCG) for Cache-Augmented Generation (CAG) in Tiny LLMs Continuous Learning Pipeline</title>
    <meta name="title" content="Membria: Knowledge Cache Graph (KCG) for Cache-Augmented Generation (CAG) in Tiny LLMs Continuous Learning Pipeline">
    <meta name="description" content="The rapid adoption of lightweight, on-device Large Language Models (Tiny LLMs) has created a critical bottleneck in personalization, knowledge...">

    <!-- Canonical -->
    <link rel="canonical" href="https://docs.actiq.xyz/knowledgecache/">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://docs.actiq.xyz/knowledgecache/">
    <meta property="og:title" content="Membria: Knowledge Cache Graph (KCG) for Cache-Augmented Generation (CAG) in Tiny LLMs Continuous Learning Pipeline">
    <meta property="og:description" content="The rapid adoption of lightweight, on-device Large Language Models (Tiny LLMs) has created a critical bottleneck in personalization, knowledge...">
    <meta property="og:image" content="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfgq_byivHQ01DpDdRaeJEg_Q8PI-KtKiFtJ4hWmuCYOYiPZGUt34LAH0EiRY1JAasviADLhiAC1eBbCH2gBQZK3AesyZsB0rkVBnZArwqiiNP8af4_EIEIZhVKueSPRkGfBU_tRw?key=AsJEkgePh24159X10uUz6PJ-">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://docs.actiq.xyz/knowledgecache/">
    <meta property="twitter:title" content="Membria: Knowledge Cache Graph (KCG) for Cache-Augmented Generation (CAG) in Tiny LLMs Continuous Learning Pipeline">
    <meta property="twitter:description" content="The rapid adoption of lightweight, on-device Large Language Models (Tiny LLMs) has created a critical bottleneck in personalization, knowledge...">
    <meta property="twitter:image" content="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfgq_byivHQ01DpDdRaeJEg_Q8PI-KtKiFtJ4hWmuCYOYiPZGUt34LAH0EiRY1JAasviADLhiAC1eBbCH2gBQZK3AesyZsB0rkVBnZArwqiiNP8af4_EIEIZhVKueSPRkGfBU_tRw?key=AsJEkgePh24159X10uUz6PJ-">

    <script data-cfasync="false">(function () { var el = document.documentElement, m = localStorage.getItem("doc_theme"), wm = window.matchMedia; if (m === "dark" || (!m && wm && wm("(prefers-color-scheme: dark)").matches)) { el.classList.add("dark") } else { el.classList.remove("dark") } })();</script>

    <link href="../static/favicon.png" rel="icon">
    <link href="../resources/css/retype.css?v=3.8.1.800899148656" rel="stylesheet">

    <script data-cfasync="false" src="../resources/js/config.js?v=3.8.1.800899148656" data-turbo-eval="false" defer></script>
    <script data-cfasync="false" src="../resources/js/retype.js?v=3.8.1" data-turbo-eval="false" defer></script>
    <script id="lunr-js" data-cfasync="false" src="../resources/js/lunr.js?v=3.8.1.800899148656" data-turbo-eval="false" defer></script>
    <script id="prism-js" data-cfasync="false" src="../resources/js/prism.js?v=3.8.1.800899148656" defer></script>
</head>
<body>
    <div id="docs-app" class="relative text-base antialiased text-gray-700 bg-white font-body dark:bg-dark-850 dark:text-dark-300">
        <div class="absolute bottom-0 left-0 bg-gray-100 dark:bg-dark-800" style="top: 5rem; right: 50%"></div>
    
        <header id="docs-site-header" class="sticky top-0 z-30 flex w-full h-16 bg-white border-b border-gray-200 md:h-20 dark:bg-dark-850 dark:border-dark-650">
            <div class="container relative flex items-center justify-between pr-6 grow md:justify-start">
                <!-- Mobile menu button skeleton -->
                <button v-cloak class="skeleton docs-mobile-menu-button flex items-center justify-center shrink-0 overflow-hidden dark:text-white focus:outline-none rounded-full w-10 h-10 ml-3.5 md:hidden"><svg xmlns="http://www.w3.org/2000/svg" class="mb-px shrink-0" width="24" height="24" viewBox="0 0 24 24" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor"><path d="M2 4h20v2H2zM2 11h20v2H2zM2 18h20v2H2z"></path></g></svg></button>
                <div v-cloak id="docs-sidebar-toggle"></div>
        
                <!-- Logo -->
                <div class="flex items-center justify-between h-full py-2 md:w-75">
                    <div class="flex items-center px-2 md:px-6">
                        <a id="docs-site-logo" href="../" class="flex items-center leading-snug text-2xl">
                            <span class="w-10 mr-2 grow-0 shrink-0 overflow-hidden">
                                <img class="max-h-10 dark:hidden md:inline-block" src="../static/light-logo.svg">
                                <img class="max-h-10 hidden dark:inline-block" src="../static/dark-logo.svg">
                            </span>
                            <span class="dark:text-white font-bold line-clamp-1 md:line-clamp-2">Actiq</span>
                        </a><span class="hidden px-2 py-1 ml-4 text-sm font-semibold leading-none text-root-logo-label-text bg-root-logo-label-bg rounded-sm md:inline-block">Litepaper</span>
                    </div>
        
                    <span class="hidden h-8 border-r md:inline-block dark:border-dark-650"></span>
                </div>
        
                <div class="flex justify-between md:grow">
                    <!-- Top Nav -->
                    <nav class="hidden md:flex">
                        <ul class="flex flex-col mb-4 md:pl-16 md:mb-0 md:flex-row md:items-center">
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="../">
                                    <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                                        <g fill="currentColor">
                                            <path d="M11.03 2.59a1.501 1.501 0 0 1 1.94 0l7.5 6.363a1.5 1.5 0 0 1 .53 1.144V19.5a1.5 1.5 0 0 1-1.5 1.5h-5.75a.75.75 0 0 1-.75-.75V14h-2v6.25a.75.75 0 0 1-.75.75H4.5A1.5 1.5 0 0 1 3 19.5v-9.403c0-.44.194-.859.53-1.144ZM12 3.734l-7.5 6.363V19.5h5v-6.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v6.25h5v-9.403Z"/>
                                        </g>
                                    </svg>
                                    <span>Home</span>
                                </a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://discord.gg/TQDtydDPgH" target="_blank">
                                    <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                                        <g fill="currentColor">
                                            <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z"/><path d="M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z"/>
                                        </g>
                                    </svg>
                                    <span>Discord</span>
                                </a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://github.com/actiquest-dev/">
                                    <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                                        <g fill="currentColor">
                                            <path d="M12 1C5.9225 1 1 5.9225 1 12C1 16.8675 4.14875 20.9787 8.52125 22.4362C9.07125 22.5325 9.2775 22.2025 9.2775 21.9137C9.2775 21.6525 9.26375 20.7862 9.26375 19.865C6.5 20.3737 5.785 19.1912 5.565 18.5725C5.44125 18.2562 4.905 17.28 4.4375 17.0187C4.0525 16.8125 3.5025 16.3037 4.42375 16.29C5.29 16.2762 5.90875 17.0875 6.115 17.4175C7.105 19.0812 8.68625 18.6137 9.31875 18.325C9.415 17.61 9.70375 17.1287 10.02 16.8537C7.5725 16.5787 5.015 15.63 5.015 11.4225C5.015 10.2262 5.44125 9.23625 6.1425 8.46625C6.0325 8.19125 5.6475 7.06375 6.2525 5.55125C6.2525 5.55125 7.17375 5.2625 9.2775 6.67875C10.1575 6.43125 11.0925 6.3075 12.0275 6.3075C12.9625 6.3075 13.8975 6.43125 14.7775 6.67875C16.8813 5.24875 17.8025 5.55125 17.8025 5.55125C18.4075 7.06375 18.0225 8.19125 17.9125 8.46625C18.6138 9.23625 19.04 10.2125 19.04 11.4225C19.04 15.6437 16.4688 16.5787 14.0213 16.8537C14.42 17.1975 14.7638 17.8575 14.7638 18.8887C14.7638 20.36 14.75 21.5425 14.75 21.9137C14.75 22.2025 14.9563 22.5462 15.5063 22.4362C19.8513 20.9787 23 16.8537 23 12C23 5.9225 18.0775 1 12 1Z"/>
                                        </g>
                                    </svg>
                                    <span>Github</span>
                                </a>
                            </li>
        
                        </ul>
                    </nav>
        
                    <!-- Header Right Skeleton -->
                    <div v-cloak class="flex justify-end grow skeleton">
        
                        <!-- Search input mock -->
                        <div class="relative hidden w-40 lg:block lg:max-w-sm lg:ml-auto">
                            <div class="absolute flex items-center justify-center h-full pl-3 dark:text-dark-300">
                                <svg xmlns="http://www.w3.org/2000/svg" class="icon-base" width="16" height="16" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 1px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                            </div>
                            <input class="w-full h-10 placeholder-gray-400 transition-colors duration-200 ease-in bg-gray-200 border border-transparent rounded md:text-sm hover:bg-white hover:border-gray-300 focus:outline-none focus:bg-white focus:border-gray-500 dark:bg-dark-600 dark:border-dark-600 dark:placeholder-dark-400" style="padding: 0.625rem 0.75rem 0.625rem 2rem" type="text" placeholder="Search">
                        </div>
        
                        <!-- Mobile search button -->
                        <div class="flex items-center justify-center w-10 h-10 lg:hidden">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="20" height="20" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                        </div>
        
                        <!-- Dark mode switch placeholder -->
                        <div class="w-10 h-10 lg:ml-2"></div>
        
                        <!-- History button -->
                        <div class="flex items-center justify-center w-10 h-10" style="margin-right: -0.625rem;">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="22" height="22" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><g ><path d="M12.01 6.01c-.55 0-1 .45-1 1V12a1 1 0 00.4.8l3 2.22a.985.985 0 001.39-.2.996.996 0 00-.21-1.4l-2.6-1.92V7.01c.02-.55-.43-1-.98-1z"></path><path d="M12.01 1.91c-5.33 0-9.69 4.16-10.05 9.4l-.29-.26a.997.997 0 10-1.34 1.48l1.97 1.79c.19.17.43.26.67.26s.48-.09.67-.26l1.97-1.79a.997.997 0 10-1.34-1.48l-.31.28c.34-4.14 3.82-7.41 8.05-7.41 4.46 0 8.08 3.63 8.08 8.09s-3.63 8.08-8.08 8.08c-2.18 0-4.22-.85-5.75-2.4a.996.996 0 10-1.42 1.4 10.02 10.02 0 007.17 2.99c5.56 0 10.08-4.52 10.08-10.08.01-5.56-4.52-10.09-10.08-10.09z"></path></g></g></svg>
                        </div>
                    </div>
        
                    <div v-cloak class="flex justify-end grow">
                        <div id="docs-mobile-search-button"></div>
                        <doc-search-desktop></doc-search-desktop>
        
                        <doc-theme-switch class="lg:ml-2"></doc-theme-switch>
                        <doc-history></doc-history>
                    </div>
                </div>
            </div>
        </header>
    
        <div class="container relative flex bg-white">
            <!-- Sidebar Skeleton -->
            <div v-cloak class="fixed flex flex-col shrink-0 duration-300 ease-in-out bg-white border-gray-200 sidebar top-20 w-75 border-r h-screen md:sticky transition-transform skeleton dark:bg-dark-800 dark:border-dark-650">
            
                <div class="flex items-center h-16 px-6">
                    <input class="w-full h-8 px-3 py-2 transition-colors duration-200 ease-linear bg-white border border-gray-200 rounded shadow-none text-sm focus:outline-none focus:border-gray-600 dark:bg-dark-600 dark:border-dark-600" type="text" placeholder="Filter">
                </div>
            
                <div class="pl-6 mt-1 mb-4">
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                </div>
            
                <div class="shrink-0 mt-auto bg-transparent dark:border-dark-650">
                    <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                        <span class="text-xs whitespace-nowrap">Powered by</span>
                        <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                    </a>
                </div>
            </div>
            
            <!-- Sidebar component -->
            <doc-sidebar v-cloak>
                <template #sidebar-footer>
                    <div class="shrink-0 mt-auto border-t md:bg-transparent md:border-none dark:border-dark-650">
            
                        <div class="py-3 px-6 md:hidden border-b dark:border-dark-650">
                            <nav>
                                <ul class="flex flex-wrap justify-center items-center">
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="../">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                                                <g fill="currentColor">
                                                    <path d="M11.03 2.59a1.501 1.501 0 0 1 1.94 0l7.5 6.363a1.5 1.5 0 0 1 .53 1.144V19.5a1.5 1.5 0 0 1-1.5 1.5h-5.75a.75.75 0 0 1-.75-.75V14h-2v6.25a.75.75 0 0 1-.75.75H4.5A1.5 1.5 0 0 1 3 19.5v-9.403c0-.44.194-.859.53-1.144ZM12 3.734l-7.5 6.363V19.5h5v-6.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v6.25h5v-9.403Z"/>
                                                </g>
                                            </svg>
                                            <span>Home</span>
                                        </a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://discord.gg/TQDtydDPgH" target="_blank">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                                                <g fill="currentColor">
                                                    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z"/><path d="M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z"/>
                                                </g>
                                            </svg>
                                            <span>Discord</span>
                                        </a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://github.com/actiquest-dev/">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mb-px mr-1" width="18" height="18" viewBox="0 0 24 24" role="presentation">
                                                <g fill="currentColor">
                                                    <path d="M12 1C5.9225 1 1 5.9225 1 12C1 16.8675 4.14875 20.9787 8.52125 22.4362C9.07125 22.5325 9.2775 22.2025 9.2775 21.9137C9.2775 21.6525 9.26375 20.7862 9.26375 19.865C6.5 20.3737 5.785 19.1912 5.565 18.5725C5.44125 18.2562 4.905 17.28 4.4375 17.0187C4.0525 16.8125 3.5025 16.3037 4.42375 16.29C5.29 16.2762 5.90875 17.0875 6.115 17.4175C7.105 19.0812 8.68625 18.6137 9.31875 18.325C9.415 17.61 9.70375 17.1287 10.02 16.8537C7.5725 16.5787 5.015 15.63 5.015 11.4225C5.015 10.2262 5.44125 9.23625 6.1425 8.46625C6.0325 8.19125 5.6475 7.06375 6.2525 5.55125C6.2525 5.55125 7.17375 5.2625 9.2775 6.67875C10.1575 6.43125 11.0925 6.3075 12.0275 6.3075C12.9625 6.3075 13.8975 6.43125 14.7775 6.67875C16.8813 5.24875 17.8025 5.55125 17.8025 5.55125C18.4075 7.06375 18.0225 8.19125 17.9125 8.46625C18.6138 9.23625 19.04 10.2125 19.04 11.4225C19.04 15.6437 16.4688 16.5787 14.0213 16.8537C14.42 17.1975 14.7638 17.8575 14.7638 18.8887C14.7638 20.36 14.75 21.5425 14.75 21.9137C14.75 22.2025 14.9563 22.5462 15.5063 22.4362C19.8513 20.9787 23 16.8537 23 12C23 5.9225 18.0775 1 12 1Z"/>
                                                </g>
                                            </svg>
                                            <span>Github</span>
                                        </a>
                                    </li>
            
                                </ul>
                            </nav>
                        </div>
            
                        <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                            <span class="text-xs whitespace-nowrap">Powered by</span>
                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                        </a>
                    </div>
                </template>
            </doc-sidebar>
    
            <div class="grow min-w-0 dark:bg-dark-850">
                <!-- Render "toolbar" template here on api pages --><!-- Render page content -->
                <div class="flex">
                    <div class="min-w-0 p-4 grow md:px-16">
                        <main class="relative pb-12 lg:pt-2">
                            <div class="docs-markdown" id="docs-content">
                                <!-- Rendered if sidebar right is enabled -->
                                <div id="docs-sidebar-right-toggle"></div>
                                <!-- Page content  -->
<doc-anchor-target id="membria-knowledge-cache-graph-kcg-for-cache-augmented-generation-cag-in-tiny-llms-continuous-learning-pipeline" class="break-words">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#membria-knowledge-cache-graph-kcg-for-cache-augmented-generation-cag-in-tiny-llms-continuous-learning-pipeline">#</doc-anchor-trigger>
        <span>Membria: Knowledge Cache Graph (KCG) for Cache-Augmented Generation (CAG) in Tiny LLMs Continuous Learning Pipeline</span>
    </h1>
</doc-anchor-target>
<p><figure class="content-center">
    <img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfgq_byivHQ01DpDdRaeJEg_Q8PI-KtKiFtJ4hWmuCYOYiPZGUt34LAH0EiRY1JAasviADLhiAC1eBbCH2gBQZK3AesyZsB0rkVBnZArwqiiNP8af4_EIEIZhVKueSPRkGfBU_tRw?key=AsJEkgePh24159X10uUz6PJ-" alt="">
    <figcaption class="caption"></figcaption>
</figure>
</p>
<hr>
<doc-anchor-target id="executive-summary">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#executive-summary">#</doc-anchor-trigger>
        <span>Executive Summary</span>
    </h1>
</doc-anchor-target>
<p>The rapid adoption of lightweight, on-device Large Language Models (Tiny LLMs) has created a critical bottleneck in personalization, knowledge freshness, and reasoning capabilities. While millions of users deploy these models for everyday tasks, their ability to learn, update, and reason remains limited, requiring costly and slow interventions via centralized Big LLM APIs or fine-tuning.</p>
<p>We introduce the <strong>Knowledge Cache Graph (KCG)</strong> and <strong>Cache-Augmented Generation (CAG)</strong> — a decentralized, verifiable, and efficient knowledge reasoning framework. Instead of relying on retraining or direct weight manipulation, KCG+CAG enables <strong>Distillation on Demand (DoD)</strong>, where knowledge is distilled, validated, and recorded in a public, immutable knowledge cache. This approach empowers Tiny LLMs to dynamically retrieve, reason over, and consume high-quality, validated knowledge via fast <strong>Selective Contextual Reasoning (SCR)</strong> pipelines, dramatically reducing inference costs, latency, and vendor lock-in.</p>
<p>By creating an ecosystem where <strong>knowledge grows, self-validates, and becomes reusable</strong>, KCG+CAG transforms AI reasoning into an open, democratic, and self-improving infrastructure for millions of Tiny LLMs.</p>
<doc-anchor-target id="problem-statement">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#problem-statement">#</doc-anchor-trigger>
        <span>Problem Statement</span>
    </h1>
</doc-anchor-target>
<p>The rise of Tiny LLMs — lightweight, on-device large language models — is transforming the AI landscape by bringing generative intelligence to billions of devices. However, this explosion of local inference introduces a fundamental bottleneck in personalization, knowledge freshness, and continuous learning:</p>
<ul>
<li><strong>Static and outdated models</strong>: Once deployed, Tiny LLMs quickly become stale, as they cannot natively update their knowledge base or integrate new information without retraining.</li>
<li><strong>Costly and centralized fine-tuning</strong>: Existing methods for updating models, such as LoRA or QLoRA fine-tuning, require cloud GPUs, expert intervention, and significant time and money.</li>
<li><strong>Dependency on Big LLM APIs</strong>: Users and apps frequently fall back on Big LLM APIs (GPT, Claude, Gemini) to access fresh or complex knowledge, incurring high costs and introducing privacy, latency, and control issues.</li>
<li><strong>Absence of a shared, reusable knowledge memory</strong>: Tiny LLMs operate in isolation, lacking access to a federated, verified, and continuously growing knowledge layer they can rely on.</li>
</ul>
<p>These limitations block the scalability of Tiny LLM reasoning capabilities, fragment knowledge across devices, and create inefficiencies both for users and for the broader AI ecosystem.</p>
<p>There is a critical need for a new approach where Tiny LLMs can <strong>dynamically acquire, reason over, and integrate fresh, verified knowledge</strong> — without retraining, without centralization, and without sacrificing privacy or efficiency.</p>
<hr>
<doc-anchor-target id="proposed-solution-overview">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#proposed-solution-overview">#</doc-anchor-trigger>
        <span>Proposed Solution Overview</span>
    </h1>
</doc-anchor-target>
<p>To overcome the personalization and reasoning bottleneck for Tiny LLMs, we propose the <strong>Knowledge Cache Graph (KCG)</strong> combined with <strong>Cache-Augmented Generation (CAG)</strong> — a decentralized knowledge reasoning layer designed for scalable, efficient, and continuous learning without retraining.</p>
<doc-anchor-target id="knowledge-cache-graph-kcg">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#knowledge-cache-graph-kcg">#</doc-anchor-trigger>
        <span>Knowledge Cache Graph (KCG)</span>
    </h2>
</doc-anchor-target>
<p>KCG is a decentralized, immutable, and verifiable knowledge graph layer, built on top of permanent storage solutions like Arweave or IPFS. It stores:</p>
<ul>
<li>Distilled knowledge entries (facts, QA, reasoning chains).</li>
<li>Verified entity relations and semantic links.</li>
<li>Embedded key-value caches for fast retrieval.</li>
<li>Proof-of-knowledge metadata ensuring data integrity and consensus validation.</li>
</ul>
<doc-anchor-target id="cache-augmented-generation-cag">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#cache-augmented-generation-cag">#</doc-anchor-trigger>
        <span>Cache-Augmented Generation (CAG)</span>
    </h2>
</doc-anchor-target>
<p>CAG introduces a <strong>Cache-Augmented Generation pipeline</strong> where Tiny LLMs no longer rely solely on RAG (retrieval-augmented generation) or direct inference from Big LLMs. Instead:</p>
<ul>
<li>Tiny LLMs first query <strong>local or Gateway KV caches</strong>, pre-filled from the KCG layer.</li>
<li>Utilize <strong>Selective Contextual Reasoning (SCR)</strong> pipelines to reason over retrieved knowledge without invoking external APIs.</li>
<li>Fallback to <strong>Distillation on Demand (DoD)</strong> requests to Big LLMs only when necessary, ensuring minimal usage of expensive inference services.</li>
</ul>
<doc-anchor-target id="distillation-on-demand-dod">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#distillation-on-demand-dod">#</doc-anchor-trigger>
        <span>Distillation on Demand (DoD)</span>
    </h2>
</doc-anchor-target>
<p>DoD allows Tiny LLMs and DoD Agents to trigger <strong>on-demand distillation of new knowledge</strong> when gaps or outdated data are detected:</p>
<ul>
<li>Distilled knowledge is submitted to Gateways.</li>
<li>Gateways validate, package, and record the knowledge into KCG.</li>
<li>This ensures that knowledge becomes reusable, validated, and available to all network participants.</li>
</ul>
<doc-anchor-target id="key-benefits-of-kcgcag">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#key-benefits-of-kcgcag">#</doc-anchor-trigger>
        <span>Key Benefits of KCG+CAG</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>Continuous, lightweight learning</strong> for Tiny LLMs without retraining.</li>
<li><strong>Dramatically reduced inference costs and latency</strong>, by leveraging fast local and Gateway caching.</li>
<li><strong>Decentralized, shared, and verifiable knowledge memory</strong>, fostering ecosystem-wide efficiency.</li>
<li><strong>Open, democratized reasoning layer</strong>, removing reliance on centralized AI providers.</li>
</ul>
<p>This model empowers Tiny LLMs to stay fresh, relevant, and capable — at the edge, in real-time, and with minimal costs.</p>
<hr>
<doc-anchor-target id="technical-architecture-overview">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#technical-architecture-overview">#</doc-anchor-trigger>
        <span>Technical Architecture Overview</span>
    </h1>
</doc-anchor-target>
<p>The KCG+CAG ecosystem is composed of several interconnected layers and actors, forming a decentralized, efficient, and verifiable knowledge reasoning pipeline optimized for Tiny LLMs.</p>
<doc-anchor-target id="components-overview">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#components-overview">#</doc-anchor-trigger>
        <span>Components Overview</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="knowledge-cache-graph-kcg-1">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#knowledge-cache-graph-kcg-1">#</doc-anchor-trigger>
        <span>Knowledge Cache Graph (KCG)</span>
    </h3>
</doc-anchor-target>
<ul>
<li><strong>Immutable Knowledge Layer</strong>: Stored on Arweave/IPFS.</li>
<li><strong>Stores</strong>:
<ul>
<li>Distilled knowledge entries (facts, QA pairs, reasoning chains).</li>
<li>Semantic relations and ontologies.</li>
<li>KV-cache entries for fast retrieval (keyed by embeddings, queries, or topics).</li>
<li>Proof-of-knowledge and validator signatures.</li>
</ul>
</li>
<li><strong>Public and verifiable</strong>: Accessible by any Gateway, agent, or user.</li>
</ul>
<doc-anchor-target id="cache-augmented-generation-cag-layer">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#cache-augmented-generation-cag-layer">#</doc-anchor-trigger>
        <span>Cache-Augmented Generation (CAG) Layer</span>
    </h3>
</doc-anchor-target>
<ul>
<li><strong>Reasoning Layer</strong> on top of KCG and KV-caches.</li>
<li><strong>Enables Tiny LLMs to perform fast retrieval and reasoning over verified knowledge without retraining</strong>.</li>
<li>Utilizes <strong>Selective Contextual Reasoning (SCR)</strong> pipelines:
<ul>
<li>Semantic retrieval.</li>
<li>Filtering and confirmation.</li>
<li>Context-enriched generation.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="distillation-on-demand-dod-agents">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#distillation-on-demand-dod-agents">#</doc-anchor-trigger>
        <span>Distillation on Demand (DoD) Agents</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Specialized agents or Tiny LLMs acting as initiators of DoD queries.</li>
<li>Responsible for detecting outdated knowledge, gaps, or novel queries.</li>
<li>Orchestrate SCR reasoning pipelines.</li>
<li>Propose distilled knowledge entries to Gateways for validation.</li>
</ul>
<doc-anchor-target id="gateways">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#gateways">#</doc-anchor-trigger>
        <span>Gateways</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Federated nodes acting as intermediaries between DoD agents and KCG.</li>
<li>Responsible for:
<ul>
<li>Knowledge validation and packaging.</li>
<li>Recording entries into KCG.</li>
<li>Managing off-chain semantic indexes and vector stores for ultra-fast retrieval.</li>
<li>Serving enriched knowledge and SCR-ready prompts to Tiny LLMs and agents.</li>
</ul>
</li>
<li>Gateways control write-access to KCG and enforce quality standards.</li>
</ul>
<doc-anchor-target id="validator-nodes">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#validator-nodes">#</doc-anchor-trigger>
        <span>Validator Nodes</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Participate in consensus voting, knowledge verification, and proof-of-knowledge processes.</li>
<li>Ensure correctness, prevent spam, and provide auditability.</li>
<li>Optionally contribute to off-chain indexing or SCR pre-computation services.</li>
</ul>
<doc-anchor-target id="hybrid-kv-cache-architecture">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#hybrid-kv-cache-architecture">#</doc-anchor-trigger>
        <span>Hybrid KV-Cache Architecture</span>
    </h3>
</doc-anchor-target>
<ul>
<li><strong>On-device Tiny LLM KV-cache</strong>: Fast, personalized cache of frequently used knowledge.</li>
<li><strong>Gateway KV-cache</strong>: High-performance, shared caching of validated knowledge and reasoning shortcuts.</li>
<li><strong>Public KV-layer in KCG</strong>: Immutable, community-validated cache ensuring knowledge persistence and transparency.</li>
</ul>
<hr>
<p>This modular, multi-layered architecture ensures:</p>
<ul>
<li><strong>Tiny LLMs can reason, learn, and update dynamically without retraining.</strong></li>
<li><strong>Gateways and validators enforce knowledge quality and validation.</strong></li>
<li><strong>Caching at multiple layers reduces inference costs, latency, and dependence on Big LLMs.</strong></li>
</ul>
<hr>
<doc-anchor-target id="workflow-overview">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#workflow-overview">#</doc-anchor-trigger>
        <span>Workflow Overview</span>
    </h1>
</doc-anchor-target>
<p>The KCG+CAG system introduces an optimized workflow that allows Tiny LLMs to acquire, reason over, and integrate fresh knowledge without the need for retraining. The process leverages multi-layer caching, Selective Contextual Reasoning (SCR), and Distillation on Demand (DoD) to ensure knowledge freshness, verifiability, and efficiency.</p>
<doc-anchor-target id="full-knowledge-lifecycle-workflow">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#full-knowledge-lifecycle-workflow">#</doc-anchor-trigger>
        <span>Full Knowledge Lifecycle Workflow</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="1-dod-request-trigger">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#1-dod-request-trigger">#</doc-anchor-trigger>
        <span>1. DoD Request Trigger</span>
    </h3>
</doc-anchor-target>
<ul>
<li>A Tiny LLM or DoD Agent identifies a knowledge gap, outdated fact, or complex reasoning need.</li>
<li>It initiates a <strong>DoD query</strong>, requesting knowledge retrieval, validation, or distillation.</li>
</ul>
<doc-anchor-target id="2-scr-reasoning-pipeline-via-gateway">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#2-scr-reasoning-pipeline-via-gateway">#</doc-anchor-trigger>
        <span>2. SCR Reasoning Pipeline via Gateway</span>
    </h3>
</doc-anchor-target>
<ul>
<li>The DoD Agent or Tiny LLM triggers the <strong>Selective Contextual Reasoning (SCR)</strong> pipeline via the Gateway:
<ul>
<li><strong>Semantic retrieval</strong> from the KCG KV-layer and Gateway off-chain index.</li>
<li><strong>Filtering and confirmation step</strong> performed locally by the agent or Tiny LLM.</li>
<li><strong>Contextual reasoning</strong> using enriched prompt with retrieved knowledge.</li>
</ul>
</li>
<li>If sufficient reasoning is possible using the local or Gateway caches, the process completes without invoking Big LLMs.</li>
</ul>
<doc-anchor-target id="3-fallback-to-big-llm-if-necessary">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#3-fallback-to-big-llm-if-necessary">#</doc-anchor-trigger>
        <span>3. Fallback to Big LLM (if necessary)</span>
    </h3>
</doc-anchor-target>
<ul>
<li>For novel, ambiguous, or high-confidence-required queries, the DoD Agent escalates the request to selected Big LLM APIs (GPT, Claude, Gemini, etc.).</li>
<li>Multiple models may be queried and compared.</li>
</ul>
<doc-anchor-target id="4-knowledge-distillation-proposal">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#4-knowledge-distillation-proposal">#</doc-anchor-trigger>
        <span>4. Knowledge Distillation Proposal</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Based on SCR or Big LLM outputs, the DoD Agent synthesizes a <strong>distilled knowledge proposal</strong>.</li>
<li>The proposal includes:
<ul>
<li>Summary.</li>
<li>Sources and evidence.</li>
<li>Semantic relations and context.</li>
<li>Optional embeddings for KV-caching.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="5-gateway-validation-and-recording">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#5-gateway-validation-and-recording">#</doc-anchor-trigger>
        <span>5. Gateway Validation and Recording</span>
    </h3>
</doc-anchor-target>
<ul>
<li>The proposal is submitted to the Gateway.</li>
<li>The Gateway performs validation steps:
<ul>
<li>Verifies data integrity and formatting.</li>
<li>Checks evidence, references, and originality.</li>
<li>Optionally invokes Validator consensus for high-value knowledge.</li>
</ul>
</li>
<li>If validated, the Gateway records the entry into the KCG (Arweave/IPFS).</li>
<li>A corresponding <strong>KV entry is updated in the public KCG KV-layer</strong> for efficient future retrieval.</li>
</ul>
<doc-anchor-target id="6-confirmation-and-reward-distribution">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#6-confirmation-and-reward-distribution">#</doc-anchor-trigger>
        <span>6. Confirmation and Reward Distribution</span>
    </h3>
</doc-anchor-target>
<ul>
<li>The DoD Agent receives the finalized KCG TXID as confirmation.</li>
<li>Validators and Gateways receive incentives for their role.</li>
<li>A portion of tokens from the DoD query are burned to maintain deflationary tokenomics.</li>
</ul>
<doc-anchor-target id="key-optimizations-in-the-workflow">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#key-optimizations-in-the-workflow">#</doc-anchor-trigger>
        <span>Key Optimizations in the Workflow</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>SCR-first by default</strong> reduces calls to Big LLM by up to 80%.</li>
<li><strong>Multi-layer caching</strong> ensures fastest possible retrieval for repeated or similar queries.</li>
<li><strong>Federated Gateway layer</strong> enables scaling and redundancy of knowledge retrieval and validation services.</li>
<li><strong>Immutable KCG layer</strong> ensures global, shared, and verifiable knowledge memory for all participants.</li>
</ul>
<hr>
<doc-anchor-target id="roles-and-responsibilities">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#roles-and-responsibilities">#</doc-anchor-trigger>
        <span>Roles and Responsibilities</span>
    </h1>
</doc-anchor-target>
<p>The KCG+CAG ecosystem is supported by distinct roles, each playing a critical part in ensuring the quality, efficiency, and scalability of the knowledge reasoning pipeline.</p>
<doc-anchor-target id="dod-agents">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#dod-agents">#</doc-anchor-trigger>
        <span>DoD Agents</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>Role</strong>: Orchestrators of Distillation on Demand (DoD) queries and reasoning pipelines.</li>
<li><strong>Responsibilities</strong>:
<ul>
<li>Identify knowledge gaps or outdated information.</li>
<li>Initiate SCR reasoning pipelines.</li>
<li>Aggregate outputs from SCR, local caches, and Big LLM APIs.</li>
<li>Propose distilled knowledge entries for validation.</li>
<li>Use and maintain local KV-caches for fast reasoning.</li>
</ul>
</li>
<li><strong>Incentives</strong>:
<ul>
<li>Earn rewards for initiating valuable DoD queries.</li>
<li>Benefit from lower inference costs via SCR and caching.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="gateways-1">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#gateways-1">#</doc-anchor-trigger>
        <span>Gateways</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>Role</strong>: Federated nodes serving as intermediaries between agents and the KCG.</li>
<li><strong>Responsibilities</strong>:
<ul>
<li>Validate, package, and record knowledge into KCG.</li>
<li>Manage off-chain semantic indexes and vector stores.</li>
<li>Operate KV-caching services for Tiny LLMs and agents.</li>
<li>Enforce quality and anti-spam measures.</li>
<li>Distribute SCR-ready prompts and knowledge to agents.</li>
</ul>
</li>
<li><strong>Incentives</strong>:
<ul>
<li>Earn transaction fees and validation rewards.</li>
<li>Maintain economic sustainability by hosting caching and reasoning infrastructure.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="validator-nodes-1">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#validator-nodes-1">#</doc-anchor-trigger>
        <span>Validator Nodes</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>Role</strong>: Guardians of knowledge correctness and consensus.</li>
<li><strong>Responsibilities</strong>:
<ul>
<li>Participate in voting and validation of proposed knowledge entries.</li>
<li>Ensure factual correctness, source verification, and semantic consistency.</li>
<li>Provide audit trails and proof-of-knowledge attestations.</li>
<li>Optionally participate in dispute resolution and governance processes.</li>
</ul>
</li>
<li><strong>Incentives</strong>:
<ul>
<li>Earn validation rewards from token flows.</li>
<li>Receive staking benefits and governance rights.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="off-chain-indexing-nodes-optional-layer">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#off-chain-indexing-nodes-optional-layer">#</doc-anchor-trigger>
        <span>Off-Chain Indexing Nodes (Optional Layer)</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>Role</strong>: Specialized nodes supporting Gateways.</li>
<li><strong>Responsibilities</strong>:
<ul>
<li>Maintain lightweight off-chain semantic graphs and vector indexes.</li>
<li>Serve SCR pipelines with ultra-fast retrieval.</li>
<li>Optimize query routing and ranking.</li>
</ul>
</li>
<li><strong>Incentives</strong>:
<ul>
<li>Earn rewards from Gateway operators or network treasury.</li>
</ul>
</li>
</ul>
<hr>
<doc-anchor-target id="knowledge-cache-graph-kcg-data-model--indexing">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#knowledge-cache-graph-kcg-data-model--indexing">#</doc-anchor-trigger>
        <span>Knowledge Cache Graph (KCG) Data Model &amp; Indexing</span>
    </h1>
</doc-anchor-target>
<p>The Knowledge Cache Graph (KCG) serves as the immutable, decentralized knowledge memory layer for the ecosystem. It is optimized to store, retrieve, and reason over distilled knowledge efficiently, while ensuring verifiability and traceability.</p>
<doc-anchor-target id="kcg-data-types">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#kcg-data-types">#</doc-anchor-trigger>
        <span>KCG Data Types</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="distilled-knowledge-entries">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#distilled-knowledge-entries">#</doc-anchor-trigger>
        <span>Distilled Knowledge Entries</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Core units of validated knowledge.</li>
<li>Contain:
<ul>
<li>Distilled summaries.</li>
<li>Supporting evidence and sources.</li>
<li>Metadata (creation date, proposer, validator signatures).</li>
<li>Optional embeddings for KV-layer indexing.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="entities">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#entities">#</doc-anchor-trigger>
        <span>Entities</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Discrete concepts, objects, or named entities.</li>
<li>Serve as graph nodes for semantic linking.</li>
<li>Example: <code v-pre>&quot;Sparse Transformer&quot;</code>, <code v-pre>&quot;Switch Transformer&quot;</code>.</li>
</ul>
<doc-anchor-target id="relations">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#relations">#</doc-anchor-trigger>
        <span>Relations</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Semantic links between entities and knowledge entries.</li>
<li>Types:
<ul>
<li><code v-pre>isA</code></li>
<li><code v-pre>relatedTo</code></li>
<li><code v-pre>usedBy</code></li>
</ul>
</li>
<li>Include descriptions, confidence scores, and provenance data.</li>
</ul>
<doc-anchor-target id="reasoning-chains">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#reasoning-chains">#</doc-anchor-trigger>
        <span>Reasoning Chains</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Multi-step logical explanations or derivations.</li>
<li>Stored as ordered steps with supporting evidence.</li>
<li>Enable Tiny LLMs to shortcut complex reasoning by using pre-validated chains.</li>
</ul>
<doc-anchor-target id="faq-patterns">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#faq-patterns">#</doc-anchor-trigger>
        <span>FAQ Patterns</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Standardized question-answer pairs.</li>
<li>Cover frequent queries and enable ultra-fast retrieval for common reasoning needs.</li>
</ul>
<doc-anchor-target id="data-format">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#data-format">#</doc-anchor-trigger>
        <span>Data Format</span>
    </h2>
</doc-anchor-target>
<ul>
<li>All data is stored as <strong>JSON-LD</strong> following linked data principles.</li>
<li>Ensures compatibility with semantic web standards and decentralized querying.</li>
</ul>
<doc-anchor-target id="indexing-strategies">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#indexing-strategies">#</doc-anchor-trigger>
        <span>Indexing Strategies</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="kv-cache-layer">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#kv-cache-layer">#</doc-anchor-trigger>
        <span>KV-Cache Layer</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Distilled entries and reasoning chains are indexed by:
<ul>
<li>Query embeddings.</li>
<li>Canonical questions.</li>
<li>Topics and categories.</li>
</ul>
</li>
<li>This layer supports ultra-fast retrieval by agents and Tiny LLMs.</li>
</ul>
<doc-anchor-target id="semantic-graph-indexes">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#semantic-graph-indexes">#</doc-anchor-trigger>
        <span>Semantic Graph Indexes</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Gateways and off-chain nodes maintain semantic graphs of relations and entities.</li>
<li>These indexes support complex reasoning paths and chaining queries.</li>
</ul>
<doc-anchor-target id="proof-of-knowledge-and-metadata">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#proof-of-knowledge-and-metadata">#</doc-anchor-trigger>
        <span>Proof-of-Knowledge and Metadata</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Each entry contains:
<ul>
<li>TXID from Arweave/IPFS (content-addressed, immutable).</li>
<li>Validator signatures and consensus proofs.</li>
<li>Timestamps and provenance records.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="key-benefits">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#key-benefits">#</doc-anchor-trigger>
        <span>Key Benefits</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>Efficient retrieval</strong> for Tiny LLMs without requiring expensive Big LLM inference.</li>
<li><strong>Verifiability and immutability</strong> via Arweave/IPFS storage.</li>
<li><strong>Semantic reasoning-ready</strong> via graph relations and reasoning chains.</li>
<li><strong>Optimized for caching and reuse</strong> via KV-layer and embeddings.</li>
</ul>
<hr>
<doc-anchor-target id="advanced-caching-strategies--scr-pipeline">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#advanced-caching-strategies--scr-pipeline">#</doc-anchor-trigger>
        <span>Advanced Caching Strategies &amp; SCR Pipeline</span>
    </h1>
</doc-anchor-target>
<p>To maximize reasoning efficiency, minimize inference costs, and enable dynamic knowledge integration for Tiny LLMs, the KCG+CAG architecture incorporates advanced caching strategies enhanced by <strong>Selective Contextual Reasoning (SCR)</strong> pipelines.</p>
<doc-anchor-target id="selective-contextual-reasoning-scr">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#selective-contextual-reasoning-scr">#</doc-anchor-trigger>
        <span>Selective Contextual Reasoning (SCR)</span>
    </h2>
</doc-anchor-target>
<p>Inspired by state-of-the-art research (arXiv:2503.05212), SCR enables Tiny LLMs to perform <strong>lightweight, dynamic reasoning over external knowledge caches without modifying model weights</strong>.</p>
<ul>
<li><strong>Step 1: Semantic Retrieval</strong> — Retrieve relevant knowledge entries from the KV-cache and semantic graph indexes.</li>
<li><strong>Step 2: Confirmation and Filtering</strong> — Tiny LLMs or Gateways filter, confirm, and deduplicate retrieved entries, ensuring contextual fit and factual accuracy.</li>
<li><strong>Step 3: Contextual Reasoning</strong> — Construct an enriched prompt using confirmed knowledge, allowing Tiny LLMs to perform high-quality reasoning locally.</li>
<li><strong>Step 4: Fallback to DoD and Big LLMs</strong> — Only if SCR fails or lacks confidence, a DoD escalation is triggered.</li>
</ul>
<doc-anchor-target id="hybrid-kv-cache-architecture-1">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#hybrid-kv-cache-architecture-1">#</doc-anchor-trigger>
        <span>Hybrid KV-Cache Architecture</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="on-device-tiny-llm-kv-cache">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#on-device-tiny-llm-kv-cache">#</doc-anchor-trigger>
        <span>On-Device Tiny LLM KV-Cache</span>
    </h3>
</doc-anchor-target>
<ul>
<li><strong>Location</strong>: Directly on user devices.</li>
<li><strong>Purpose</strong>: Fast, personalized retrieval for frequent or user-specific knowledge.</li>
<li><strong>Latency</strong>: Sub-20 ms access time.</li>
</ul>
<doc-anchor-target id="gateway-kv-cache">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#gateway-kv-cache">#</doc-anchor-trigger>
        <span>Gateway KV-Cache</span>
    </h3>
</doc-anchor-target>
<ul>
<li><strong>Location</strong>: Gateways or local edge servers.</li>
<li><strong>Purpose</strong>: Shared, community-level cache of validated knowledge and reasoning chains.</li>
<li><strong>Latency</strong>: 50–200 ms access time.</li>
</ul>
<doc-anchor-target id="public-kv-layer-in-kcg">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#public-kv-layer-in-kcg">#</doc-anchor-trigger>
        <span>Public KV-Layer in KCG</span>
    </h3>
</doc-anchor-target>
<ul>
<li><strong>Location</strong>: Immutable storage (Arweave/IPFS).</li>
<li><strong>Purpose</strong>: Community-validated, permanent cache of distilled knowledge and reasoning blocks.</li>
<li><strong>Latency</strong>: 300–1000 ms (direct access), faster via Gateway indexes.</li>
</ul>
<doc-anchor-target id="gateway-off-chain-index-layer">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#gateway-off-chain-index-layer">#</doc-anchor-trigger>
        <span>Gateway Off-Chain Index Layer</span>
    </h2>
</doc-anchor-target>
<ul>
<li>Gateways maintain lightweight off-chain semantic graphs, vector indexes, and retrieval services.</li>
<li>These indexes accelerate SCR pipelines, enabling sub-second retrieval even from large, decentralized knowledge graphs.</li>
<li>They also serve as a <strong>Federated Knowledge Mesh</strong>, ensuring redundancy, locality, and resilience.</li>
</ul>
<doc-anchor-target id="key-benefits-of-scr-and-advanced-caching">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#key-benefits-of-scr-and-advanced-caching">#</doc-anchor-trigger>
        <span>Key Benefits of SCR and Advanced Caching</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>Up to 80% of reasoning queries served locally or via Gateway SCR pipelines</strong>, drastically reducing Big LLM API calls.</li>
<li><strong>Dynamic reasoning capabilities without retraining or fine-tuning</strong>.</li>
<li><strong>Privacy-preserving local reasoning with fallback to decentralized DoD processes</strong>.</li>
<li><strong>Efficient caching and routing optimized for Tiny LLM environments</strong>.</li>
</ul>
<p>By combining SCR with multi-layered caching, KCG+CAG establishes an <strong>agile, self-learning reasoning infrastructure</strong>, enabling Tiny LLMs to stay fresh, responsive, and efficient at the edge.</p>
<hr>
<doc-anchor-target id="gateway-reasoning-orchestration--knowledge-gap-detection">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#gateway-reasoning-orchestration--knowledge-gap-detection">#</doc-anchor-trigger>
        <span>Gateway Reasoning Orchestration &amp; Knowledge Gap Detection</span>
    </h2>
</doc-anchor-target>
<p>The Gateway is more than a passive storage and query endpoint — it actively orchestrates the quality and structure of the Knowledge Cache Graph (KCG). Its core responsibility is to detect reasoning gaps, manage KV-caching layers, and coordinate the creation of new knowledge via DoD agents.</p>
<doc-anchor-target id="detecting-knowledge-gaps-and-hot-topics">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#detecting-knowledge-gaps-and-hot-topics">#</doc-anchor-trigger>
        <span>Detecting Knowledge Gaps and Hot Topics</span>
    </h3>
</doc-anchor-target>
<p>Gateways continuously monitor query traffic and KV cache behavior to detect:</p>
<ul>
<li>Frequent KV misses: repeated user queries where no matching cached KV exists.</li>
<li>Spikes in similar queries: indicating a trend or emerging interest (e.g. &quot;fine-tuning LLaMA&quot;, &quot;RAG vs CAG&quot;).</li>
<li>Redundant calls to Big LLMs: same queries triggering repeated costly API calls — a sign of missing reusable reasoning.</li>
<li>Stale or outdated KV blocks: old distillations no longer consistent with updated knowledge graphs or ontologies.</li>
</ul>
<p>This results in the identification of Knowledge Hotspots or Reasoning Deficits.</p>
<doc-anchor-target id="kv-cache-orchestration">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#kv-cache-orchestration">#</doc-anchor-trigger>
        <span>KV Cache Orchestration</span>
    </h3>
</doc-anchor-target>
<p>The Gateway maintains a multi-tiered cache structure:</p>
<ul>
<li>Hot Layer: High-frequency, high-importance KV blocks.</li>
<li>Warm Layer: Medium-use, non-volatile knowledge.</li>
<li>Cold Layer: Low-use, outdated, or weakly relevant KV blocks.</li>
</ul>
<p>Each entry is annotated with:</p>
<ul>
<li><code v-pre>importance_score</code>: provided by DoD agent or inferred via usage analytics.</li>
<li><code v-pre>last_accessed</code> timestamp.</li>
<li><code v-pre>source_confidence</code> and <code v-pre>validation_passed</code>.</li>
</ul>
<p>Based on these signals, the Gateway:</p>
<ul>
<li>Ranks new KV entries using a hybrid scoring model (importance × frequency × freshness).</li>
<li>Evicts or offloads stale entries to disk/off-chain cold storage.</li>
<li>Recalls pages of KV blocks when relevant topics re-emerge (recency-based paging).</li>
<li>Bundles KV-chains and reasoning paths for replay and reconstruction.</li>
</ul>
<doc-anchor-target id="proactive-distillation-task-creation">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#proactive-distillation-task-creation">#</doc-anchor-trigger>
        <span>Proactive Distillation Task Creation</span>
    </h3>
</doc-anchor-target>
<p>When a pattern of deficiency is detected, the Gateway generates and queues distillation tasks:</p>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-json"><code v-pre class="language-json">{
  &quot;query&quot;: &quot;how to train LLaMA on custom data&quot;,
  &quot;pattern_id&quot;: &quot;llama_ft_2024&quot;,
  &quot;trigger&quot;: &quot;miss_rate&gt;80%&quot;,
  &quot;task_type&quot;: &quot;proactive_distillation&quot;,
  &quot;bounty&quot;: 4,
  &quot;requirements&quot;: {
    &quot;agent_status&quot;: &quot;idle&quot;,
    &quot;gpu&quot;: true,
    &quot;max_latency&quot;: &quot;5s&quot;
  }
}</code></pre>
</doc-codeblock></div>
<p>These tasks are broadcast to DoD Agents that meet the resource and status conditions.</p>
<doc-anchor-target id="intelligent-kv-layering-and-feedback-loop">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#intelligent-kv-layering-and-feedback-loop">#</doc-anchor-trigger>
        <span>Intelligent KV Layering and Feedback Loop</span>
    </h3>
</doc-anchor-target>
<p>As agents submit new KV blocks and reasoning chains:</p>
<ul>
<li>Gateway scores and validates them.</li>
<li>High-confidence blocks are pushed to the hot layer.</li>
<li>Usage data is fed back to reinforce or decay importance scores.</li>
</ul>
<p>This feedback loop ensures self-improvement of the reasoning layer over time — without requiring global retraining of any model.</p>
<doc-anchor-target id="outcome">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#outcome">#</doc-anchor-trigger>
        <span>Outcome</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Better reuse of reasoning → lower Big LLM cost.</li>
<li>Dynamic adaptation of the knowledge graph to user demand.</li>
<li>Coordinated, decentralized creation of new verified knowledge via task-market economics.</li>
</ul>
<hr>
<doc-anchor-target id="incentivized-task-routing-proactive-dod-distillation-from-gateway">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#incentivized-task-routing-proactive-dod-distillation-from-gateway">#</doc-anchor-trigger>
        <span>Incentivized Task Routing: Proactive DoD Distillation from Gateway</span>
    </h2>
</doc-anchor-target>
<p>To ensure the quality and freshness of knowledge in the KCG (Knowledge Cache Graph), Gateways can proactively request knowledge distillation - even when no user explicitly triggers it. This is especially useful for trending queries or emerging knowledge gaps.</p>
<doc-anchor-target id="why-gateways-initiate-proactive-dod-tasks">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#why-gateways-initiate-proactive-dod-tasks">#</doc-anchor-trigger>
        <span>Why Gateways Initiate Proactive DoD Tasks</span>
    </h3>
</doc-anchor-target>
<p>Gateways detect:</p>
<ul>
<li>Frequent KV cache misses for a specific query pattern</li>
<li>High-volume repeated API calls to Big LLMs for the same topic</li>
<li>Insufficient reasoning coverage in the current KCG</li>
</ul>
<p>They then generate a <strong>distillation task</strong> with a canonical query and metadata, and broadcast it to available DoD Agents.</p>
<doc-anchor-target id="what-a-distillation-task-looks-like">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#what-a-distillation-task-looks-like">#</doc-anchor-trigger>
        <span>What a Distillation Task Looks Like</span>
    </h3>
</doc-anchor-target>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-json"><code v-pre class="language-json">{
  &quot;type&quot;: &quot;distillation_request&quot;,
  &quot;triggered_by&quot;: &quot;kv_miss_frequency&quot;,
  &quot;canonical_query&quot;: &quot;how to fine-tune LLaMA&quot;,
  &quot;priority&quot;: &quot;high&quot;,
  &quot;bounty&quot;: 3,
  &quot;requirements&quot;: {
    &quot;agent_status&quot;: &quot;idle&quot;,
    &quot;min_gpu_mem&quot;: &quot;12GB&quot;,
    &quot;response_time&quot;: &quot;&lt;10s&quot;
  }
}</code></pre>
</doc-codeblock></div>
<doc-anchor-target id="distillation-tasks-incentivization-for-dod-agents">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#distillation-tasks-incentivization-for-dod-agents">#</doc-anchor-trigger>
        <span>Distillation tasks Incentivization for DoD Agents?</span>
    </h3>
</doc-anchor-target>
<p>Options include:</p>
<ul>
<li><strong>Gateway treasury</strong> funded from user token fees</li>
<li><strong>Task-level bounties</strong> paid to the first agent who completes and passes validation</li>
<li><strong>Enterprise sponsors</strong> funding custom knowledge domains</li>
</ul>
<p>Agents only earn rewards if their results:</p>
<ul>
<li>Pass semantic and factual validation</li>
<li>Are accepted into the KCG by the Gateway</li>
<li>Remain cached beyond a minimum usage threshold</li>
</ul>
<doc-anchor-target id="dod-agent-task-acceptance-protocol">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#dod-agent-task-acceptance-protocol">#</doc-anchor-trigger>
        <span>DoD Agent Task Acceptance Protocol</span>
    </h3>
</doc-anchor-target>
<p>Agents may <strong>accept or reject</strong> tasks based on:</p>
<ul>
<li>Device status (CPU/GPU/TPU load, memory, battery state etc.)</li>
<li>Recent task queue</li>
<li>Reward threshold</li>
</ul>
<doc-anchor-target id="key-benefits-1">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#key-benefits-1">#</doc-anchor-trigger>
        <span>Key Benefits</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Incentivized load balancing across edge agents</li>
<li>Minimal latency for hot-topic knowledge retrieval</li>
<li>Reduced redundant Big LLM calls</li>
<li>Efficient use of idle resources</li>
</ul>
<p>This model transforms Gateway nodes into <strong>active coordinators of knowledge</strong>, while DoD Agents become <strong>economic actors in an open market of reasoning services</strong>.</p>
<hr>
<doc-anchor-target id="context-window-optimization">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#context-window-optimization">#</doc-anchor-trigger>
        <span>Context Window Optimization</span>
    </h2>
</doc-anchor-target>
<p>Tiny LLMs, while efficient and portable, are often limited by their small context windows (512–2048 tokens). To make Cache-Augmented Generation (CAG) viable on such models, we implement intelligent strategies to ensure only the most relevant, high-value information is loaded into the prompt. This optimization maximizes the quality of reasoning without exceeding memory or latency budgets.</p>
<doc-anchor-target id="design-segmented-kv-buffer">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#design-segmented-kv-buffer">#</doc-anchor-trigger>
        <span>Design: Segmented KV Buffer</span>
    </h3>
</doc-anchor-target>
<p>We introduce a <strong>Segmented Key-Value Buffer</strong> within the DoD Agent or runtime environment:</p>
<ul>
<li><strong>Static Core Slot:</strong> Long-term, frequently accessed domain facts (e.g. atomic knowledge)</li>
<li><strong>Dynamic Slot:</strong> Fetched knowledge highly relevant to the current query (determined via semantic similarity or attention hinting)</li>
<li><strong>Recall Slot:</strong> Recently used paths in reasoning, offering continuity across turns</li>
</ul>
<p>At each generation step, a runtime scheduler assembles the prompt buffer from these segments, prioritizing tokens under a fixed token limit.</p>
<doc-anchor-target id="importance-scoring--prioritized-paging">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#importance-scoring--prioritized-paging">#</doc-anchor-trigger>
        <span>Importance Scoring &amp; Prioritized Paging</span>
    </h3>
</doc-anchor-target>
<p>Each cached item (text chunk or KV pair) is ranked using:</p>
<ul>
<li>Relevance to query (via embedding similarity)</li>
<li>Usage frequency (historical reuse)</li>
<li>Role in reasoning chains (e.g., root → conclusion → supporting node)</li>
</ul>
<p>This scoring ensures that only knowledge with <strong>highest utility</strong> is included in inference prompts.</p>
<doc-anchor-target id="compression-of-knowledge-items">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#compression-of-knowledge-items">#</doc-anchor-trigger>
        <span>Compression of Knowledge Items</span>
    </h3>
</doc-anchor-target>
<p>To fit more content per token budget:</p>
<ul>
<li><strong>Summarization models</strong> (e.g., MiniLM or TinyBERT) are used to produce compact versions of longer documents</li>
<li>Knowledge is reformatted into <strong>bullet points, entity-relationship triples</strong>, or <strong>graph edges</strong></li>
<li>Chain-of-thought reasoning is <strong>condensed</strong> into minimal logical steps</li>
</ul>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-json"><code v-pre class="language-json">{
  &quot;concept&quot;: &quot;photosynthesis&quot;,
  &quot;summary&quot;: &quot;Plants convert light into energy via chlorophyll.&quot;,
  &quot;facts&quot;: [&quot;Needs sunlight&quot;, &quot;Produces O2&quot;, &quot;Occurs in chloroplasts&quot;]
}</code></pre>
</doc-codeblock></div>
<doc-anchor-target id="rollover-scheduling--memory-swapping">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#rollover-scheduling--memory-swapping">#</doc-anchor-trigger>
        <span>Rollover Scheduling &amp; Memory Swapping</span>
    </h3>
</doc-anchor-target>
<p>When a query requires more tokens than the model can ingest:</p>
<ul>
<li>The DoD Agent conducts <strong>multi-pass inference</strong></li>
<li>Each pass feeds different segments of the relevant cache</li>
<li>Final answers are <strong>aggregated and reconciled</strong>, preserving accuracy while reducing load</li>
</ul>
<doc-anchor-target id="benefits-for-users">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#benefits-for-users">#</doc-anchor-trigger>
        <span>Benefits for Users</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Higher answer quality, with focused, relevant facts</li>
<li>Fewer hallucinations, due to grounding in verified cache</li>
<li>Faster response time, by avoiding bloated, irrelevant context</li>
<li>Better personalization, as the buffer can adapt to user history or device profile</li>
</ul>
<p>This context-aware design unlocks the full power of CAG even on low-resource, edge-deployed LLMs, making them viable agents of real-time reasoning.</p>
<hr>
<doc-anchor-target id="segmented-kv-buffer--prioritized-paging">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#segmented-kv-buffer--prioritized-paging">#</doc-anchor-trigger>
        <span>Segmented KV Buffer &amp; Prioritized Paging</span>
    </h2>
</doc-anchor-target>
<p>To support efficient reasoning and memory management, each DoD Agent maintains a <strong>Segmented KV Buffer</strong> — a multi-layered cache that mirrors the mental model of short-term memory, long-term knowledge, and shared intelligence.</p>
<p>This buffer is not a flat list of KV pairs but a <strong>prioritized, dynamic structure</strong> divided by scope and usage intent.</p>
<doc-anchor-target id="kv-buffer-segments">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#kv-buffer-segments">#</doc-anchor-trigger>
        <span>KV Buffer Segments</span>
    </h3>
</doc-anchor-target>
<ol>
<li><p><strong>Session Memory</strong></p>
<ul>
<li>Stores recent tokens, prompts, and in-context outputs.</li>
<li>Volatile and specific to the current user/task.</li>
<li>Used for continuity in short dialogs and multi-turn queries.</li>
</ul>
</li>
<li><p><strong>Local Knowledge Cache</strong></p>
<ul>
<li>Stores previously distilled or reused knowledge from past tasks.</li>
<li>Retrieved from prior DoD calls or localized user storage.</li>
<li>Adaptively refreshed based on reuse frequency.</li>
</ul>
</li>
<li><p><strong>Global Shared KV Layer (KCG-derived)</strong></p>
<ul>
<li>Contains verified reasoning blocks from the Gateway or global KCG.</li>
<li>Pulled in lazily or preloaded based on semantic match with query.</li>
<li>Immutable and tagged with metadata (source, confidence, TTL).</li>
</ul>
</li>
</ol>
<p>Each segment has its own memory policy: TTL, max size, eviction rules, and update frequency.</p>
<doc-anchor-target id="paging-and-prioritization-logic">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#paging-and-prioritization-logic">#</doc-anchor-trigger>
        <span>Paging and Prioritization Logic</span>
    </h3>
</doc-anchor-target>
<p>Before executing inference, the DoD Agent performs:</p>
<ul>
<li><strong>Semantic prefiltering</strong>: scoring available KV entries based on relevance to the current prompt/query.</li>
<li><strong>Priority ranking</strong>: weighting entries by segment (session &gt; local &gt; global), recency, and confidence.</li>
<li><strong>Page selection</strong>: choosing top-N blocks to load into active KV memory.</li>
<li><strong>KV hydration</strong>: loading precomputed values directly into the model’s attention cache.</li>
</ul>
<p>If memory is constrained (e.g., GPU VRAM), paging strategies are applied:</p>
<ul>
<li><strong>Evict least recently used (LRU)</strong> entries first.</li>
<li><strong>Drop low-confidence or low-impact chains</strong>.</li>
<li><strong>Recall previously offloaded KV blocks from disk or host memory</strong> if needed.</li>
</ul>
<doc-anchor-target id="benefits-for-reasoning">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#benefits-for-reasoning">#</doc-anchor-trigger>
        <span>Benefits for Reasoning</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Reduces prompt size by reusing memory instead of refeeding tokens.</li>
<li>Speeds up inference via prehydrated KV attention.</li>
<li>Enables long-form or multi-stage reasoning without exceeding context limits.</li>
<li>Supports “memory-based personalization” without model fine-tuning.</li>
</ul>
<doc-anchor-target id="real-user-value">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#real-user-value">#</doc-anchor-trigger>
        <span>Real User Value</span>
    </h3>
</doc-anchor-target>
<p>For end users, this means:</p>
<ul>
<li><strong>Faster answers</strong> — as the model skips redundant context and loads memory directly.</li>
<li><strong>Cheaper queries</strong> — fewer tokens sent to Big LLM APIs or fewer cycles burned locally.</li>
<li><strong>Smarter responses over time</strong> — as the agent remembers what it learned and reuses it.</li>
<li><strong>No lag during long tasks</strong> — complex queries feel instant, even on edge devices.</li>
</ul>
<doc-anchor-target id="future-optimizations">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#future-optimizations">#</doc-anchor-trigger>
        <span>Future Optimizations</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Learning-based context routers to optimize KV selection dynamically.</li>
<li>Attention-aware scheduling: align paging with expected model read patterns.</li>
<li>Collaborative KV: agents share anonymized hot blocks via Gateway to bootstrap each other’s reasoning.</li>
</ul>
<p>This architecture transforms DoD Agents from stateless callers into <strong>adaptive, memory-efficient reasoning units</strong>, capable of high-quality inference under local constraints — and delivering visible gains in speed, cost, and usefulness to users.</p>
<hr>
<doc-anchor-target id="comparative-table-membria-vs-leading-llm-learning--adaptation-solutions">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#comparative-table-membria-vs-leading-llm-learning--adaptation-solutions">#</doc-anchor-trigger>
        <span>Comparative Table: Membria vs. Leading LLM Learning &amp; Adaptation Solutions</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="overview">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#overview">#</doc-anchor-trigger>
        <span>Overview</span>
    </h3>
</doc-anchor-target>
<p>As the demand for more private, efficient, and customizable AI grows, the market has responded with a variety of solutions—from centralized APIs to retrieval-augmented generation (RAG) systems and lightweight finetuning frameworks. However, most approaches either depend on constant cloud interaction, lack persistence, or do not offer real-time reasoning improvements for on-device LLMs.</p>
<p><strong>Membria</strong> introduces a new paradigm: Cache-Augmented Generation (CAG), combining structured, validated memory (KCG), real-time distillation (DoD Agents), and edge-native inference. This comparative table benchmarks Membria against seven of the most prominent systems currently in the market.</p>
<ul>
<li><strong>Gemini Nano (Google):</strong> Uses limited RAG from on-device search, no persistent memory or caching, short context windows, inference-only.</li>
<li><strong>GPT-4 with RAG (OpenAI):</strong> Long context, document retrieval at inference time, but no persistent cache or KV buffer reuse.</li>
<li><strong>GPTCache:</strong> Semantic caching plugin that reuses full responses based on similarity, but lacks reasoning-path control or KV structure.</li>
<li><strong>Hugging Face Transformers + PEFT:</strong> Custom finetuning pipelines, no built-in runtime KV cache logic.</li>
<li><strong>Cohere Embed &amp; RAG API:</strong> Offers embeddings + vector retrieval, but no on-device precomputed cache structure.</li>
</ul>
<doc-anchor-target id="comparative-table">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#comparative-table">#</doc-anchor-trigger>
        <span>Comparative Table</span>
    </h3>
</doc-anchor-target>
<div class="table-wrapper scrollbar overflow-hidden">
<table class="comfortable">
<thead>
<tr>
<th>Feature / Platform</th>
<th><strong>Membria</strong></th>
<th>Amazon Bedrock</th>
<th>Google Vertex AI (Gemini)</th>
<th>Hugging Face AutoTrain</th>
<th>GPTCache</th>
<th>Cohere API</th>
<th>AI21 Labs</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Deployment Model</strong></td>
<td>On-device / no-cloud</td>
<td>Cloud-hosted</td>
<td>Cloud + Edge (Pixel)</td>
<td>Cloud-based</td>
<td>Plugin-based</td>
<td>API-based</td>
<td>API-based</td>
</tr>
<tr>
<td><strong>Learning Paradigm</strong></td>
<td>CAG + DoD + Shared KCG</td>
<td>Offline distillation</td>
<td>RAG + prompting</td>
<td>LoRA/PEFT finetuning</td>
<td>Response caching</td>
<td>Embedding + RAG</td>
<td>Embedding + RAG</td>
</tr>
<tr>
<td><strong>Real-Time Adaptation</strong></td>
<td><span class="docs-emoji">&#x2705;</span> Yes</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Limited</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Similar queries</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
</tr>
<tr>
<td><strong>Inference Location</strong></td>
<td><span class="docs-emoji">&#x2705;</span> Fully local</td>
<td><span class="docs-emoji">&#x274C;</span> Cloud</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Partial (Gemini Nano)</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Optional on-prem</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> User-defined</td>
<td><span class="docs-emoji">&#x274C;</span> Cloud</td>
<td><span class="docs-emoji">&#x274C;</span> Cloud</td>
</tr>
<tr>
<td><strong>Persistent Memory Layer</strong></td>
<td><span class="docs-emoji">&#x2705;</span> Yes (KCG)</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> In-memory only</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
</tr>
<tr>
<td><strong>Context Optimization</strong></td>
<td><span class="docs-emoji">&#x2705;</span> Segmented KV + Prioritized Paging</td>
<td><span class="docs-emoji">&#x274C;</span> None</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Prompt structuring</td>
<td><span class="docs-emoji">&#x274C;</span> None</td>
<td><span class="docs-emoji">&#x274C;</span> None</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Some filtering</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Some filtering</td>
</tr>
<tr>
<td><strong>Offline Capability</strong></td>
<td><span class="docs-emoji">&#x2705;</span> Yes</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Partial</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x2705;</span> If local</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
<td><span class="docs-emoji">&#x274C;</span> No</td>
</tr>
<tr>
<td><strong>Reasoning Enhancements</strong></td>
<td><span class="docs-emoji">&#x2705;</span> CAG + Chain Condensation</td>
<td><span class="docs-emoji">&#x274C;</span> None</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Embedding-based</td>
<td><span class="docs-emoji">&#x274C;</span> None</td>
<td><span class="docs-emoji">&#x274C;</span> None</td>
<td><span class="docs-emoji">&#x274C;</span> None</td>
<td><span class="docs-emoji">&#x274C;</span> None</td>
</tr>
<tr>
<td><strong>Knowledge Sharing</strong></td>
<td><span class="docs-emoji">&#x2705;</span> Public/Private KCG</td>
<td><span class="docs-emoji">&#x274C;</span> Closed</td>
<td><span class="docs-emoji">&#x274C;</span> Closed</td>
<td><span class="docs-emoji">&#x274C;</span> Per model</td>
<td><span class="docs-emoji">&#x274C;</span> Local only</td>
<td><span class="docs-emoji">&#x274C;</span> Closed</td>
<td><span class="docs-emoji">&#x274C;</span> Closed</td>
</tr>
<tr>
<td><strong>Personalization Strategy</strong></td>
<td><span class="docs-emoji">&#x2705;</span> Local + Shared memory</td>
<td><span class="docs-emoji">&#x274C;</span> None</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Google Workspace only</td>
<td><span class="docs-emoji">&#x2705;</span> Finetuning</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Basic tuning</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Prompt tuning</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Prompt tuning</td>
</tr>
<tr>
<td><strong>Cost Efficiency</strong></td>
<td><span class="docs-emoji">&#x2705;</span> Very high (no tokens, local)</td>
<td><span class="docs-emoji">&#x274C;</span> Expensive cloud</td>
<td><span class="docs-emoji">&#x274C;</span> API metered</td>
<td><span class="docs-emoji">&#x26A0;&#xFE0F;</span> Medium</td>
<td><span class="docs-emoji">&#x2705;</span> High</td>
<td><span class="docs-emoji">&#x274C;</span> Subscription</td>
<td><span class="docs-emoji">&#x274C;</span> Subscription</td>
</tr>
</tbody>
</table>
</div>
<doc-anchor-target id="summary">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#summary">#</doc-anchor-trigger>
        <span>Summary</span>
    </h3>
</doc-anchor-target>
<p>Membria is the only solution in this landscape that combines:</p>
<ul>
<li><strong>Real-time, on-device inference and learning</strong></li>
<li><strong>Structured knowledge caching with validation</strong></li>
<li><strong>Cost-efficient and private reasoning</strong></li>
<li><strong>Scalable, persistent memory usable by multiple agents</strong></li>
</ul>
<p>Other systems focus on cloud delivery, static models, or inference-time hacks (e.g., caching or RAG), without providing a framework for <strong>ongoing, distributed intelligence</strong>. Membria’s architecture offers a truly decentralized foundation for the next generation of lightweight, smart AI agents.</p>
<hr>
<doc-anchor-target id="integration-with-peak-protocol">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#integration-with-peak-protocol">#</doc-anchor-trigger>
        <span>Integration with PEAK Protocol</span>
    </h1>
</doc-anchor-target>
<p>The KCG+CAG system can be seamlessly integrated and deployed on top of <strong>PEAK Protocol</strong>, leveraging its existing decentralized infrastructure, consensus mechanisms, and privacy-enhancing features.</p>
<doc-anchor-target id="peak-enabled-components">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#peak-enabled-components">#</doc-anchor-trigger>
        <span>PEAK-Enabled Components</span>
    </h2>
</doc-anchor-target>
<div class="table-wrapper scrollbar overflow-hidden">
<table class="comfortable">
<thead>
<tr>
<th>KCG+CAG Component</th>
<th>Integration with PEAK Protocol</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>KCG On-Chain Storage &amp; TX</td>
<td><span class="docs-emoji">&#x2705;</span> Use PEAK Chain for transaction recording and consensus</td>
<td>Knowledge entries, validator approvals, governance votes</td>
</tr>
<tr>
<td>ZK Proofs for DoD &amp; Access</td>
<td><span class="docs-emoji">&#x2705;</span> Use PEAK ZK Layer for privacy-preserving queries &amp; attestation</td>
<td>ZK Proof-of-Access, ZK validation of query authorization</td>
</tr>
<tr>
<td>Off-chain Indexing &amp; Subgraphs</td>
<td><span class="docs-emoji">&#x2705;</span> Use PEAK Subgraph Infrastructure for accelerated querying</td>
<td>Gateways can offload indexing to PEAK Subgraph tooling</td>
</tr>
<tr>
<td>SCR Pipelines &amp; Reasoning</td>
<td>KCG+CAG Layer</td>
<td>Application-specific layer on top of PEAK</td>
</tr>
<tr>
<td>Local Tiny LLM KV Cache</td>
<td>Device or Gateway Level</td>
<td>Fast inference on device or at gateway edge nodes</td>
</tr>
<tr>
<td>DoD Agents and CAG Pipelines</td>
<td>KCG+CAG Layer</td>
<td>Orchestrate reasoning and distillation above PEAK</td>
</tr>
</tbody>
</table>
</div>
<doc-anchor-target id="benefits-of-deploying-kcgcag-over-peak-protocol">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#benefits-of-deploying-kcgcag-over-peak-protocol">#</doc-anchor-trigger>
        <span>Benefits of Deploying KCG+CAG over PEAK Protocol</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>Validator Network and Governance</strong>: KCG can adopt PEAK&#x27;s validator network, staking mechanisms, and DAO governance, reducing the need to bootstrap a separate validator economy.</li>
<li><strong>ZK Proof Layer</strong>: PEAK&#x27;s ZK Layer can serve as the foundation for query privacy, access control, and proof-of-knowledge attestations within KCG and DoD requests.</li>
<li><strong>Optimized Querying and Indexing</strong>: By utilizing PEAK&#x27;s subgraph and indexing services, KCG can enhance its off-chain semantic querying layer, accelerating SCR pipelines without duplicating infrastructure.</li>
</ul>
<doc-anchor-target id="layered-architecture-overview">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#layered-architecture-overview">#</doc-anchor-trigger>
        <span>Layered Architecture Overview</span>
    </h2>
</doc-anchor-target>
<div class="codeblock-wrapper"><doc-codeblock>
<pre class="language-none"><code v-pre class="language-none">Application &amp; Agent Layer
└── Tiny LLMs, User Devices, Enterprise Agents

CAG Reasoning Layer (on top of PEAK)
└── Cache-Augmented Generation, SCR Pipelines, DoD Agents, Gateway Cache &amp; Index

KCG Core Layer (on PEAK)
└── Knowledge Cache Graph, Distilled Knowledge, Relations, KV Layer (on-chain)

PEAK Protocol Layer
└── Validator Network, Consensus, Staking, ZK Proof Layer, Subgraph Indexing</code></pre>
</doc-codeblock></div>
<hr>
<doc-anchor-target id="tokenomics--incentive-design">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#tokenomics--incentive-design">#</doc-anchor-trigger>
        <span>Tokenomics &amp; Incentive Design</span>
    </h1>
</doc-anchor-target>
<p>The KCG+CAG ecosystem introduces a deflationary, utility-driven token model designed to reward key participants, sustain decentralized infrastructure, and ensure long-term economic balance.</p>
<doc-anchor-target id="token-flows">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#token-flows">#</doc-anchor-trigger>
        <span>Token Flows</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="dod-query-payments">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#dod-query-payments">#</doc-anchor-trigger>
        <span>DoD Query Payments</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Every <strong>Distillation on Demand (DoD) request</strong> initiated by a DoD Agent incurs a fixed token fee.</li>
<li>This covers:
<ul>
<li>API calls to Big LLMs (if needed).</li>
<li>Storage costs (Arweave, off-chain index updates).</li>
<li>Validation and recording into KCG.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="validator--gateway-rewards">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#validator--gateway-rewards">#</doc-anchor-trigger>
        <span>Validator &amp; Gateway Rewards</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Validators and Gateways are compensated in tokens for:
<ul>
<li>Validating knowledge proposals.</li>
<li>Maintaining KV-caches and SCR pipelines.</li>
<li>Hosting off-chain indexes and retrieval services.</li>
</ul>
</li>
<li>Rewards are split proportionally based on workload, reputation, and node performance.</li>
</ul>
<doc-anchor-target id="token-burning-mechanism">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#token-burning-mechanism">#</doc-anchor-trigger>
        <span>Token Burning Mechanism</span>
    </h3>
</doc-anchor-target>
<ul>
<li>A percentage of each DoD request fee is <strong>burned</strong> (removed from circulation).</li>
<li>This creates a <strong>deflationary pressure</strong>, balancing the incentive system and aligning with knowledge quality over quantity.</li>
</ul>
<doc-anchor-target id="key-economic-principles">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#key-economic-principles">#</doc-anchor-trigger>
        <span>Key Economic Principles</span>
    </h2>
</doc-anchor-target>
<div class="table-wrapper scrollbar overflow-hidden">
<table class="comfortable">
<thead>
<tr>
<th>Mechanism</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>DoD Payments (Fixed Tokens)</td>
<td>Incentivize Agents to propose valuable knowledge, sustain infrastructure costs.</td>
</tr>
<tr>
<td>Validator/Gateway Rewards</td>
<td>Compensate nodes for validation, caching, and retrieval services.</td>
</tr>
<tr>
<td>Token Burning</td>
<td>Ensure long-term deflationary pressure, avoiding unchecked token inflation.</td>
</tr>
</tbody>
</table>
</div>
<doc-anchor-target id="sustainability-strategy">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#sustainability-strategy">#</doc-anchor-trigger>
        <span>Sustainability Strategy</span>
    </h2>
</doc-anchor-target>
<ul>
<li>Incentives are designed to <strong>reward useful knowledge contribution, validation, and retrieval</strong>, not speculative behavior.</li>
<li>Over time, as the knowledge cache grows and DoD frequency decreases due to efficient SCR pipelines, the system self-balances toward lower operating costs and higher knowledge utility per token spent.</li>
</ul>
<hr>
<doc-anchor-target id="governance--validator-operations">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#governance--validator-operations">#</doc-anchor-trigger>
        <span>Governance &amp; Validator Operations</span>
    </h1>
</doc-anchor-target>
<p>The integrity, fairness, and sustainability of the KCG+CAG ecosystem are ensured by a decentralized governance model and a distributed network of Validator Nodes.</p>
<doc-anchor-target id="validators">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#validators">#</doc-anchor-trigger>
        <span>Validators</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="roles">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#roles">#</doc-anchor-trigger>
        <span>Roles</span>
    </h3>
</doc-anchor-target>
<ul>
<li><strong>Knowledge Validation</strong>: Review and approve or reject distilled knowledge proposals submitted via Gateways.</li>
<li><strong>Consensus Voting</strong>: Participate in staking-based or delegated voting mechanisms for validating high-value or disputed entries.</li>
<li><strong>Proof-of-Knowledge Verification</strong>: Sign knowledge entries, providing audit trails and trust guarantees.</li>
<li><strong>Governance Participation</strong>: Influence protocol upgrades, tokenomic adjustments, and dispute resolutions.</li>
</ul>
<doc-anchor-target id="infrastructure">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#infrastructure">#</doc-anchor-trigger>
        <span>Infrastructure</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Validators can be:
<ul>
<li><strong>Permissionless nodes</strong> operated by the community.</li>
<li><strong>Elected by governance mechanisms</strong> based on reputation, staking, or delegated trust.</li>
</ul>
</li>
<li>They run lightweight nodes capable of:
<ul>
<li>Verifying knowledge proposals.</li>
<li>Participating in voting rounds.</li>
<li>Optionally contributing to off-chain indexing and SCR pre-computation services.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="governance-model">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#governance-model">#</doc-anchor-trigger>
        <span>Governance Model</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="dao-driven-or-federated-governance">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#dao-driven-or-federated-governance">#</doc-anchor-trigger>
        <span>DAO-Driven or Federated Governance</span>
    </h3>
</doc-anchor-target>
<ul>
<li>The protocol may adopt:
<ul>
<li><strong>DAO governance models</strong>, where token holders vote on key parameters.</li>
<li><strong>Federated validator councils</strong>, ensuring fast and efficient decision-making with delegated transparency.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="responsibilities">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#responsibilities">#</doc-anchor-trigger>
        <span>Responsibilities</span>
    </h3>
</doc-anchor-target>
<ul>
<li>Protocol upgrades and parameter tuning (e.g., DoD fees, validator rewards, burn rates).</li>
<li>Validator slashing mechanisms for malicious behavior.</li>
<li>Dispute resolution between agents, validators, and gateways.</li>
<li>Treasury management and incentive pool allocations.</li>
</ul>
<doc-anchor-target id="key-governance-goals">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#key-governance-goals">#</doc-anchor-trigger>
        <span>Key Governance Goals</span>
    </h2>
</doc-anchor-target>
<ul>
<li><strong>Ensure high knowledge quality and verifiability.</strong></li>
<li><strong>Prevent spam, misinformation, and abuse.</strong></li>
<li><strong>Balance openness with integrity.</strong></li>
<li><strong>Enable community participation while ensuring operational efficiency.</strong></li>
</ul>
<hr>
<doc-anchor-target id="conclusion--vision">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#conclusion--vision">#</doc-anchor-trigger>
        <span>Conclusion &amp; Vision</span>
    </h1>
</doc-anchor-target>
<p>The KCG+CAG ecosystem bridges the gap between heavy, centralized LLM inference and lightweight, efficient Tiny LLMs operating at the edge. By introducing an open, decentralized, and verifiable knowledge graph, coupled with Cache-Augmented Generation (CAG) and Selective Contextual Reasoning (SCR), we enable Tiny LLMs to stay continuously updated, smart, and capable — without costly retraining or vendor lock-in.</p>
<p>This paradigm shift turns reasoning and knowledge augmentation into <strong>an open, reusable, and community-driven resource</strong>, breaking free from centralized control and enabling AI models to reason dynamically using decentralized knowledge caches.</p>
<doc-anchor-target id="our-vision">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#our-vision">#</doc-anchor-trigger>
        <span>Our Vision</span>
    </h2>
</doc-anchor-target>
<p>We envision a world where:</p>
<ul>
<li><strong>Tiny LLMs become truly autonomous learners</strong>, continuously improving and reasoning at the edge.</li>
<li><strong>Knowledge becomes a public good</strong>, verifiable and accessible to all, stored immutably in the Knowledge Cache Graph (KCG).</li>
<li><strong>Users, agents, and validators collaborate in a self-reinforcing ecosystem</strong>, where knowledge grows organically, costs decrease, and reasoning becomes more reliable, democratic, and sovereign.</li>
</ul>
<p>By adopting the KCG+CAG architecture, we take a significant step toward <strong>democratizing AI reasoning, decentralizing knowledge creation, and empowering users everywhere to control, enhance, and benefit from their own intelligent agents.</strong></p>
<hr>

                                
                                <!-- Required only on API pages -->
                                <doc-toolbar-member-filter-no-results></doc-toolbar-member-filter-no-results>
                            </div>
                            <footer class="clear-both">
                            
                                <nav class="flex mt-14">
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 h-full flex items-center break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-l-lg transition-colors duration-150 relative hover:z-5" href="../humanvision/">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mr-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19 11H7.41l5.29-5.29a.996.996 0 10-1.41-1.41l-7 7a1 1 0 000 1.42l7 7a1.024 1.024 0 001.42-.01.996.996 0 000-1.41L7.41 13H19c.55 0 1-.45 1-1s-.45-1-1-1z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                            <span>
                                                <span class="block text-xs font-normal text-gray-400 dark:text-dark-400">Previous</span>
                                                <span class="block mt-1">Human Vision Subnet</span>
                                            </span>
                                        </a>
                                    </div>
                            
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 -mx-px h-full flex items-center justify-end break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-r-lg transition-colors duration-150 relative hover:z-5" href="../roadmap/">
                                            <span>
                                                <span class="block text-xs font-normal text-right text-gray-400 dark:text-dark-400">Next</span>
                                                <span class="block mt-1">Roadmap</span>
                                            </span>
                                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19.92 12.38a1 1 0 00-.22-1.09l-7-7a.996.996 0 10-1.41 1.41l5.3 5.3H5c-.55 0-1 .45-1 1s.45 1 1 1h11.59l-5.29 5.29a.996.996 0 000 1.41c.19.2.44.3.7.3s.51-.1.71-.29l7-7c.09-.09.16-.21.21-.33z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                        </a>
                                    </div>
                                </nav>
                            </footer>
                        </main>
                
                        <div class="border-t dark:border-dark-650 pt-6 mb-8">
                            <footer class="flex flex-wrap items-center justify-between">
                                <div>
                                    <ul class="flex flex-wrap items-center text-sm">
                                    </ul>
                                </div>
                                <div class="docs-copyright py-2 text-gray-500 dark:text-dark-350 text-sm leading-relaxed"><p>© Copyright 2025, <a href="https://actiq.xyz">Actiquest Inc.</a> All rights reserved.</p></div>
                            </footer>
                        </div>
                    </div>
                
                    <!-- Rendered if sidebar right is enabled -->
                    <!-- Sidebar right skeleton-->
                    <div v-cloak class="fixed top-0 bottom-0 right-0 translate-x-full bg-white border-gray-200 lg:sticky lg:border-l lg:shrink-0 lg:pt-6 lg:transform-none sm:w-1/2 lg:w-64 lg:z-0 md:w-104 sidebar-right skeleton dark:bg-dark-850 dark:border-dark-650">
                        <div class="pl-5">
                            <div class="w-32 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-48 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-40 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                        </div>
                    </div>
                
                    <!-- User should be able to hide sidebar right -->
                    <doc-sidebar-right v-cloak></doc-sidebar-right>
                </div>

            </div>
        </div>
    
        <doc-search-mobile></doc-search-mobile>
        <doc-back-to-top></doc-back-to-top>
    </div>


    <div id="docs-overlay-target"></div>

    <script data-cfasync="false">window.__DOCS__ = { "title": "Knowledge Cache", level: 1, icon: "file", hasPrism: true, hasMermaid: false, hasMath: false, tocDepth: 23 }</script>
</body>
</html>
